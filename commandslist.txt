import os
import sys

spark_path = "C:\Users\skarfin\Documents\spark\spark-1.6.1-bin-hadoop2.6" 

os.environ['SPARK_HOME'] = spark_path
os.environ['HADOOP_HOME'] = spark_path

sys.path.append(spark_path + "/bin")
sys.path.append(spark_path + "/python")
sys.path.append(spark_path + "/python/pyspark/")
sys.path.append(spark_path + "/python/lib")
sys.path.append(spark_path + "/python/lib/pyspark.zip")
sys.path.append(spark_path + "/python/lib/py4j-0.9-src.zip")

from pyspark import SparkContext
from pyspark import SparkConf

sc = SparkContext("local", "test") 


autoData=sc.textFile("auto-data.csv")
autoData.cache()
autoData.count()
autoData.first()
autoData.take(5)
for line in autoData.collect();
	print line
	
output
[u'MAKE,FUELTYPE,ASPIRE,DOORS,BBODY,DRIVE,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE',
 u'subaru,gas,std,two,hatchback,fwd,four,69,4900,31,36,5118',
 u'chevrolet,gas,std,two,hatchback,fwd,three,48,5100,47,53,5195',
 u'mazda,gas,std,two,hatchback,fwd,four,68,5000,30,31,5348',
 u'toyota,gas,std,two,hatchback,fwd,four,62,4800,35,39,5389']
In [7]:

	
#map and create new rdd
toyotaData=autoData.filter(lambda x: "toyota" in x)
toyotaData.count()
output:
3

#FlatMap
words=toyotaData.flatMap(lambda line:line.split(","))
words.take(20)

collData=sc.parallelize([3,5,4,7,4])
collData.cache()
collData.count()
#distinct
for numbData in collData.distinct().collect():
	print numbData
output:
3
4
5
7

words1=sc.parallelize(["hello","war","peace","world"])
words2=sc.parallelize(["war","peace","universe"])

for unions in words1.union(words2).distinct().collect():
	print unions
output:
world
peace
hello
universe
war	
	
for intersects in words1.intersection(words2).collect();
	print intersects
	
	
autoData.reduce(lambda x,y:x if len(x)< len(y) else y)
output:
u'nissan,gas,std,two,sedan,fwd,four,69,5200,31,37,6649'


#aggregation
#perform the same work as reduce
seqOp=(lambda x,y:(x+y))
combOp=lambda x,y:(x+y))
collData.aggregate((0),seqOp,combOp)

#Do addition and multiplication at the same time
seqOp=(lambda x,y:(x[0]+y,x[1]*y))
combOp=(lambda x,y:(x[0]+y[0],x[1]+y[1]))
collData.aggregate((0,1),seqOp,combOp)

(23, 1681)

#spark function

def cleanseRDD(autoStr) :
	if isinstance(autoStr,int):
		return autoStr
	attList=autoStr.split(",")
	#convert doors to a number
	if attList[3] == "two":
		attList[3]="2"
	else :
		attList[5]="4"
	#convert Drive to uppercase
	attList[5]=att:ist[5].upper()
	return ",".join(attList)
cleanedData=autoData.map(cleanseRDD)
cleanedData.collect()

output:
[u'MAKE,FUELTYPE,ASPIRE,DOORS,BBODY,4,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE',
 u'subaru,gas,std,2,hatchback,FWD,four,69,4900,31,36,5118',
 u'chevrolet,gas,std,2,hatchback,FWD,three,48,5100,47,53,5195',
 u'mazda,gas,std,2,hatchback,FWD,four,68,5000,30,31,5348',
 u'toyota,gas,std,2,hatchback,FWD,four,62,4800,35,39,5389',
 u'mitsubishi,gas,std,2,hatchback,FWD,four,68,5500,37,41,5399',
 u'honda,gas,std,2,hatchback,FWD,four,60,5500,38,42,5399',
 u'nissan,gas,std,2,sedan,FWD,four,69,5200,31,37,5499',
 u'dodge,gas,std,2,hatchback,FWD,four,68,5500,37,41,5572',
 u'plymouth,gas,std,2,hatchback,FWD,four,68,5500,37,41,5572',
 u'mazda,gas,std,2,hatchback,FWD,four,68,5000,31,38,6095',
 u'mitsubishi,gas,std,2,hatchback,FWD,four,68,5500,31,38,6189',
 u'dodge,gas,std,four,hatchback,4,four,68,5500,31,38,6229',
 u'plymouth,gas,std,four,hatchback,4,four,68,5500,31,38,6229',
 u'chevrolet,gas,std,2,hatchback,FWD,four,70,5400,38,43,6295',
 u'toyota,gas,std,2,hatchback,FWD,four,62,4800,31,38,6338',
 u'dodge,gas,std,2,hatchback,FWD,four,68,5500,31,38,6377',
 u'honda,gas,std,four,hatchback,4,four,58,4800,49,54,6479',
 u'toyota,gas,std,2,hatchback,FWD,four,62,6800,31,38,6488',
 u'honda,gas,std,2,hatchback,FWD,four,76,6000,30,34,6529',
 u'chevrolet,gas,std,2,sedan,FWD,four,70,5400,38,43,6575',
 u'nissan,gas,std,2,sedan,FWD,four,69,5200,31,37,6649',
 u'mitsubishi,gas,std,2,hatchback,FWD,four,68,5500,31,38,6669']

	
